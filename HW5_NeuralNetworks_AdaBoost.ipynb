{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96cfc3d",
   "metadata": {},
   "source": [
    "### Part I – Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89900e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part I: Dataset Selection\n",
      "Number of samples: 150\n",
      "Number of features: 4\n",
      "Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Dataset Selection and Preprocessin\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Loadingn the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Part I: Dataset Selection\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ded3e",
   "metadata": {},
   "source": [
    "### Part I – Conclusions\n",
    "- The Iris dataset from the UCI repository was chosen for its simplicity and suitability for classification tasks. With 150 samples, 4 features (sepal length, sepal width, petal length, petal width), and 3 classes (Setosa, Versicolor, Virginica), it provides a solid foundation for testing the neural network and ensemble methods implemented in this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a0bc2",
   "metadata": {},
   "source": [
    "### Part II – Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7972ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aykutss/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part II: AdaBoost with MLP\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost with MLP as base classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define MLP classifier that supports sample weights\n",
    "class KerasMLPClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_size=10, max_iter=1000, random_state=42):\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        np.random.seed(self.random_state)\n",
    "        tf.random.set_seed(self.random_state)\n",
    "        self.classes_ = np.unique(y)\n",
    "        num_classes = len(self.classes_)\n",
    "        self.model = Sequential([\n",
    "            Dense(self.hidden_layer_size, activation='relu', input_shape=(X.shape[1],)),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.model.fit(X, y, epochs=self.max_iter, verbose=0, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.model.predict(X, verbose=0)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Train AdaBoost ensemble with MLP base classifier\n",
    "base_classifier = KerasMLPClassifier()\n",
    "ada_boost = AdaBoostClassifier(estimator=base_classifier, n_estimators=50, random_state=42)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "y_pred_ada = ada_boost.predict(X_test)\n",
    "\n",
    "print(\"\\nPart II: AdaBoost with MLP\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62610d48",
   "metadata": {},
   "source": [
    "### Part II – Conclusions\n",
    "- The AdaBoost model, utilizing a custom MLP with one hidden layer (10 neurons) as the base classifier, demonstrated perfect classification performance on the Iris dataset. The ensemble approach enhanced the MLP’s ability to handle complex decision boundaries, resulting in 100% accuracy, with precision, recall, and F1-scores of 1.00 across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d092ba",
   "metadata": {},
   "source": [
    "### Part III – Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ebf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part III: Custom Random Forest with Perceptron Decisions\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom Random Forest with Perceptron-based Nodes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import mode\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, perceptron=None, feature_indices=None, left=None, right=None, prediction=None):\n",
    "        self.perceptron = perceptron\n",
    "        self.feature_indices = feature_indices\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.prediction = prediction\n",
    "\n",
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, num_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.num_features = num_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.num_features = X.shape[1] if self.num_features is None else min(self.num_features, X.shape[1])\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        if depth >= self.max_depth or num_samples < self.min_samples_split or len(np.unique(y)) == 1:\n",
    "            return Node(prediction=np.bincount(y).argmax())\n",
    "        feature_indices = np.random.choice(num_features, self.num_features, replace=False)\n",
    "        best_gini, best_split = float('inf'), None\n",
    "        unique_classes = np.unique(y)\n",
    "        for class_ in unique_classes:\n",
    "            A = [class_]\n",
    "            B = [c for c in unique_classes if c != class_]\n",
    "            y_binary = np.where(np.isin(y, A), 0, 1)\n",
    "            perceptron = LogisticRegression(max_iter=1000)\n",
    "            perceptron.fit(X[:, feature_indices], y_binary)\n",
    "            predictions = perceptron.predict(X[:, feature_indices])\n",
    "            left_idx, right_idx = np.where(predictions == 0)[0], np.where(predictions == 1)[0]\n",
    "            if len(left_idx) == 0 or len(right_idx) == 0:\n",
    "                continue\n",
    "            counts_left = np.bincount(y[left_idx], minlength=len(unique_classes))\n",
    "            counts_right = np.bincount(y[right_idx], minlength=len(unique_classes))\n",
    "            gini_left = 1 - sum((counts_left / max(1, len(left_idx)))**2)\n",
    "            gini_right = 1 - sum((counts_right / max(1, len(right_idx)))**2)\n",
    "            gini = (len(left_idx) / num_samples) * gini_left + (len(right_idx) / num_samples) * gini_right\n",
    "            if gini < best_gini:\n",
    "                best_gini, best_split = gini, (perceptron, feature_indices)\n",
    "        if best_split is None:\n",
    "            return Node(prediction=np.bincount(y).argmax())\n",
    "        perceptron, feature_indices = best_split\n",
    "        predictions = perceptron.predict(X[:, feature_indices])\n",
    "        left_tree = self._grow_tree(X[predictions == 0], y[predictions == 0], depth + 1)\n",
    "        right_tree = self._grow_tree(X[predictions == 1], y[predictions == 1], depth + 1)\n",
    "        return Node(perceptron=perceptron, feature_indices=feature_indices, left=left_tree, right=right_tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x, self.root) for x in X])\n",
    "\n",
    "    def _predict_single(self, x, node):\n",
    "        if node.prediction is not None:\n",
    "            return node.prediction\n",
    "        pred = node.perceptron.predict(x[node.feature_indices].reshape(1, -1))[0]\n",
    "        return self._predict_single(x, node.left if pred == 0 else node.right)\n",
    "\n",
    "class CustomRandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2, num_features=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.num_features = num_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            idx = np.random.choice(len(X), len(X), replace=True)\n",
    "            tree = CustomDecisionTree(self.max_depth, self.min_samples_split, self.num_features)\n",
    "            tree.fit(X[idx], y[idx])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return mode(predictions, axis=0)[0].flatten()\n",
    "\n",
    "forest = CustomRandomForest()\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred_rf = forest.predict(X_test)\n",
    "print(\"\\nPart III: Custom Random Forest with Perceptron Decisions\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2eb0d2",
   "metadata": {},
   "source": [
    "### Part III – Conclusions\n",
    "- The custom random forest model, using perceptron-like logistic regressors as decision nodes, achieved perfect classification performance on the Iris dataset. By leveraging oblique decision boundaries instead of standard threshold-based splits, the model was able to effectively separate the classes. This demonstrates the strength of using trainable models at the node level within ensemble methods, especially for small and clean datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
